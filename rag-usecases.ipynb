{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364229ea",
   "metadata": {},
   "source": [
    "Classic RAG (Dense Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a382628",
   "metadata": {},
   "source": [
    "Plaintext:\n",
    "\n",
    "Q: What are the library opening hours?\n",
    "A: The library is open from 9am to 8pm on weekdays.\n",
    "\n",
    "Q: How do I obtain my student ID?\n",
    "A: Student IDs are issued at the administration desk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65488e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.3.61)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (3.12.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
      "Requirement already satisfied: anyio in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a662eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from faiss-cpu) (24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e5a2e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can go to the library on weekdays between 9am and 8pm.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from typing import List, Any\n",
    "\n",
    "# docs: List of tuples containing question-answer pairs.\n",
    "# Each tuple represents a document with a question and its corresponding answer.\n",
    "docs = [\n",
    "    (\"What are the library opening hours?\", \"The library is open from 9am to 8pm on weekdays.\"),\n",
    "    (\"How do I obtain my student ID?\", \"Student IDs are issued at the administration desk.\"),\n",
    "]\n",
    "\n",
    "# texts: List of formatted strings combining questions and answers.\n",
    "# Each string is formatted as \"Q: <question>\\nA: <answer>\" for embedding and retrieval.\n",
    "texts = [f\"Q: {q}\\nA: {a}\" for q, a in docs]\n",
    "\n",
    "# CustomAPIEmbeddings: Custom embedding class using your /embedding API.\n",
    "# - Inherits from LangChain's Embeddings base class.\n",
    "# - embed_documents() embeds a list of texts by calling embed_query() for each.\n",
    "# - embed_query() sends a POST request to your /embedding endpoint with the text and model version.\n",
    "# - The API returns a dense vector (embedding) for the input text.\n",
    "# - Example: The text \"Q: What are the library opening hours?\\nA: ...\" is converted into a high-dimensional vector (e.g., 1536 dimensions).\n",
    "class CustomAPIEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/embedding\",\n",
    "            json={\"input\": text, \"model_version\": \"text-embedding-ada-002\"},\n",
    "            timeout=10,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"embedding\"]\n",
    "\n",
    "# CustomAPILLM: Custom LLM class using your /chat API.\n",
    "# - Inherits from LangChain's LLM base class.\n",
    "# - _call() sends a POST request to your /chat endpoint with the system prompt, user prompt, and model version.\n",
    "# - The API returns a generated answer in the \"content\" field.\n",
    "# - Example: When a user asks \"When can I go to the library?\", the system retrieves the most relevant Q&A pair and uses the LLM to answer in context.\n",
    "class CustomAPILLM(LLM):\n",
    "    def _call(self, prompt: str, stop: List[str] = [], **kwargs: Any) -> str:\n",
    "        payload = {\n",
    "            \"system_prompt\": \"You are a helpful assistant.\",\n",
    "            \"user_prompt\": prompt,\n",
    "            \"model_version\": \"gpt-4.1-2025-04-14\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/chat\",\n",
    "            json=payload,\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"content\"]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_api_llm\"\n",
    "\n",
    "# vectorstore: FAISS vector store instance.\n",
    "# - FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.\n",
    "# - It is commonly used to quickly find similar items (e.g., documents, images) in large datasets.\n",
    "# - from_texts() creates a vector store from the provided texts using the specified embedding model.\n",
    "#   Example: If you have 1000 FAQ pairs, each will be converted to a vector and stored in FAISS for fast retrieval.\n",
    "# - CustomAPIEmbeddings generates dense vector representations (embeddings) for each text using your custom embedding API.\n",
    "# - Indexes: FAISS builds an index over these vectors, allowing fast nearest neighbor search. This index is not a traditional database index, but a structure (like an inverted file or HNSW graph) optimized for vector similarity.\n",
    "# - Vector dimensions: Each embedding is a list (array) of numbers (floats), where the length (dimension) depends on the embedding model (e.g., 1536 for ada-002). Higher dimensions can capture more semantic information.\n",
    "# - Why arrays? Vectors are arrays because mathematical operations (like dot product or cosine similarity) are performed on them to measure similarity.\n",
    "vectorstore = FAISS.from_texts(texts, CustomAPIEmbeddings())\n",
    "\n",
    "# qa: RetrievalQA chain instance.\n",
    "# - RetrievalQA is a LangChain chain that combines a retriever and a language model (LLM) for question answering.\n",
    "# - from_chain_type() initializes the chain with:\n",
    "#     - llm: The language model to generate answers (here, your custom LLM).\n",
    "#     - retriever: The retriever interface from the vector store, used to fetch relevant documents.\n",
    "# - Example: When a user asks \"When can I go to the library?\", the retriever converts the question to a vector, finds the most similar vectors (documents) in FAISS, and passes them to the LLM to generate a final answer.\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=CustomAPILLM(), retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Run the QA chain with a user query.\n",
    "# - qa.run() takes a question as input, retrieves relevant documents, and generates an answer using the LLM.\n",
    "# - Example: If the input is \"When can I go to the library?\", the system retrieves the most relevant Q&A pair and uses the LLM to answer in context.\n",
    "print(qa.run(\"When can I go to the library?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3b7f5",
   "metadata": {},
   "source": [
    "Hybrid RAG (Dense + Sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e75df",
   "metadata": {},
   "source": [
    "A Hybrid RAG system combines dense (semantic/vector) retrieval and sparse (keyword/BM25) retrieval and merges or reranks their results for improved relevance and robustness.\n",
    "\n",
    "LangChain for orchestration\n",
    "\n",
    "sentence-transformers for dense retrieval via vector search (using Chroma)\n",
    "\n",
    "rank_bm25 for sparse retrieval (BM25, pure Python!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5317054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: faiss-cpu in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: rank_bm25 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.3.61)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (3.12.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (0.32.1)\n",
      "Requirement already satisfied: Pillow in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.8.0)\n",
      "Requirement already satisfied: colorama in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\tinkererslab\\rag-usecases\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community faiss-cpu sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cda0484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVED CONTEXTS:\n",
      "- To reset the device, press and hold the power button for 10 seconds.\n",
      "- If the printer jams, open the rear door and remove the stuck paper.\n",
      "\n",
      "MODEL ANSWER:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KoliSn\\AppData\\Local\\Temp\\ipykernel_22464\\3342053413.py:112: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your device won't turn on, try resetting it by pressing and holding the power button for 10 seconds. If it still doesn't respond, make sure it is properly connected to a power source or charged, then try the reset again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n---------------------------\\nHow is this different from classical RAG (dense retrieval)?\\n---------------------------\\n\\nClassical RAG (Dense Retrieval):\\n- Uses only dense retrieval: queries and documents are embedded into vectors, and similarity search (e.g., FAISS) is used to find relevant documents.\\n- Retrieval is based on semantic similarity, not just keyword overlap.\\n- May miss relevant documents if the embedding model fails to capture certain keywords or domain-specific terms.\\n\\nHybrid RAG (Dense + Sparse):\\n- Combines dense retrieval (semantic similarity) and sparse retrieval (keyword/BM25).\\n- BM25 (sparse) excels at exact keyword matches and rare terms, while dense retrieval captures semantic meaning.\\n- Results from both retrievers are merged (and deduplicated), improving recall and robustness.\\n- Especially useful when queries are ambiguous, contain rare words, or when the embedding model is imperfect.\\n- In this code, both retrieval methods are used, and the LLM is provided with a richer, more relevant context.\\n\\nSummary:\\nHybrid RAG leverages the strengths of both retrieval paradigms, often leading to better and more reliable answers than classical (dense-only) RAG.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Any\n",
    "import requests\n",
    "\n",
    "# 1. Example data (can be replaced with your own)\n",
    "docs = [\n",
    "    \"The library is open from 9am to 8pm on weekdays.\",\n",
    "    \"Student IDs are issued at the administration desk.\",\n",
    "    \"To reset the device, press and hold the power button for 10 seconds.\",\n",
    "    \"If the printer jams, open the rear door and remove the stuck paper.\",\n",
    "]\n",
    "\n",
    "# 2. Prepare BM25 (sparse retrieval)\n",
    "# Tokenize each document for BM25, which is a keyword-based retrieval algorithm.\n",
    "doc_tokens = [doc.lower().split() for doc in docs]\n",
    "bm25 = BM25Okapi(doc_tokens)\n",
    "\n",
    "class CustomAPIEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Calls embed_query for each text (one-by-one)\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/embedding\",\n",
    "            json={\"input\": text, \"model_version\": \"text-embedding-ada-002\"},\n",
    "            timeout=10,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"embedding\"]\n",
    "\n",
    "class CustomAPILLM(LLM):\n",
    "    \"\"\"\n",
    "    Custom LLM class that uses a remote API to generate answers.\n",
    "    This replaces local LLMs with a call to your own /chat endpoint.\n",
    "    \"\"\"\n",
    "    def _call(self, prompt: str, stop: List[str] = None, **kwargs: Any) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the custom LLM API.\n",
    "        Args:\n",
    "            prompt: The prompt string to send to the LLM.\n",
    "            stop: Optional stop tokens (not used here).\n",
    "        Returns:\n",
    "            The generated answer as a string.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"system_prompt\": \"You are a helpful assistant.\",\n",
    "            \"user_prompt\": prompt,\n",
    "            \"model_version\": \"gpt-4.1-2025-04-14\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/chat\",\n",
    "            json=payload,\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"content\"]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_api_llm\"\n",
    "\n",
    "# 3. Prepare FAISS (dense retrieval) with custom embeddings\n",
    "# FAISS is a vector database for fast similarity search over dense vectors.\n",
    "# Here, we use our custom embedding API to convert docs to vectors.\n",
    "embeddings = CustomAPIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "\n",
    "def hybrid_retrieve(query: str, top_n: int = 2) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve top-N relevant documents using both sparse (BM25) and dense (FAISS) retrieval,\n",
    "    then merge results without duplicates.\n",
    "    Args:\n",
    "        query: The user query string.\n",
    "        top_n: Number of top results to retrieve from each retriever.\n",
    "    Returns:\n",
    "        List of unique relevant documents.\n",
    "    \"\"\"\n",
    "    # Sparse retrieval: BM25 scores based on keyword overlap.\n",
    "    sparse_scores = bm25.get_scores(query.lower().split())\n",
    "    sparse_indices = sorted(range(len(sparse_scores)), key=lambda i: -sparse_scores[i])[:top_n]\n",
    "    # Dense retrieval: FAISS finds semantically similar docs via embeddings.\n",
    "    dense_results = vectorstore.similarity_search(query, k=top_n)\n",
    "    dense_indices = [docs.index(res.page_content) for res in dense_results]\n",
    "    # Merge indices, preserving order and removing duplicates.\n",
    "    hybrid_indices = []\n",
    "    for idx in sparse_indices + dense_indices:\n",
    "        if idx not in hybrid_indices:\n",
    "            hybrid_indices.append(idx)\n",
    "    return [docs[idx] for idx in hybrid_indices]\n",
    "\n",
    "# Instantiate the custom LLM\n",
    "llm = CustomAPILLM()\n",
    "\n",
    "def hybrid_qa(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer a user query using hybrid retrieval-augmented generation.\n",
    "    Retrieves relevant context using both sparse and dense retrieval, then generates an answer.\n",
    "    Args:\n",
    "        query: The user question.\n",
    "    Returns:\n",
    "        The generated answer string.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant contexts using hybrid retrieval.\n",
    "    context = \"\\n\".join(hybrid_retrieve(query, top_n=2))\n",
    "    # Compose the prompt for the LLM.\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    # Generate the answer using the custom LLM.\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "# 7. Run a demo\n",
    "user_query = \"How do I reset my device if it won't turn on?\"\n",
    "print(\"RETRIEVED CONTEXTS:\")\n",
    "for ctx in hybrid_retrieve(user_query):\n",
    "    print(\"-\", ctx)\n",
    "print(\"\\nMODEL ANSWER:\")\n",
    "print(hybrid_qa(user_query))\n",
    "\n",
    "\"\"\"\n",
    "---------------------------\n",
    "How is this different from classical RAG (dense retrieval)?\n",
    "---------------------------\n",
    "\n",
    "Classical RAG (Dense Retrieval):\n",
    "- Uses only dense retrieval: queries and documents are embedded into vectors, and similarity search (e.g., FAISS) is used to find relevant documents.\n",
    "- Retrieval is based on semantic similarity, not just keyword overlap.\n",
    "- May miss relevant documents if the embedding model fails to capture certain keywords or domain-specific terms.\n",
    "\n",
    "Hybrid RAG (Dense + Sparse):\n",
    "- Combines dense retrieval (semantic similarity) and sparse retrieval (keyword/BM25).\n",
    "- BM25 (sparse) excels at exact keyword matches and rare terms, while dense retrieval captures semantic meaning.\n",
    "- Results from both retrievers are merged (and deduplicated), improving recall and robustness.\n",
    "- Especially useful when queries are ambiguous, contain rare words, or when the embedding model is imperfect.\n",
    "- In this code, both retrieval methods are used, and the LLM is provided with a richer, more relevant context.\n",
    "\n",
    "Summary:\n",
    "Hybrid RAG leverages the strengths of both retrieval paradigms, often leading to better and more reliable answers than classical (dense-only) RAG.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8138ed",
   "metadata": {},
   "source": [
    "CoT RAG ( Chain of thoughts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01761ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 LLM: Sure, let's think step by step:\n",
      "\n",
      "1. The Eiffel Tower is a famous landmark.\n",
      "2. It is located in Europe.\n",
      "3. More specifically, it is in France.\n",
      "4. Within France, it is in the capital city, Paris.\n",
      "5. In Paris, the Eiffel Tower stands on the Champ de Mars, near the Seine River.\n",
      "\n",
      "**Final answer:** The Eiffel Tower is located in Paris, France, on the Champ de Mars near the Seine River.\n",
      "Step 2 LLM: Let's think step by step:\n",
      "\n",
      "1. The Eiffel Tower is a landmark.\n",
      "2. This landmark is in Paris.\n",
      "3. Paris is the capital city of France.\n",
      "4. France is a country in Europe.\n",
      "\n",
      "So, the Eiffel Tower is located in Paris, France, which is in Europe.\n",
      "Step 3 LLM: Let's think step by step:\n",
      "\n",
      "1. The Eiffel Tower is a landmark in Paris.\n",
      "2. Paris is the capital city of France.\n",
      "3. France is a country in Europe.\n",
      "\n",
      "Therefore, the Eiffel Tower is located in Paris, France, which is in Europe.\n",
      "Final Answer: The final answer is: The Eiffel Tower is located in Paris, France, which is in Europe.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import List, Any\n",
    "\n",
    "# --- Custom LLM API for Chain-of-Thought RAG ---\n",
    "\n",
    "class CustomAPILLM:\n",
    "    \"\"\"\n",
    "    Custom LLM class that uses a remote API to generate answers.\n",
    "    This replaces local LLMs (like OpenAI) with a call to your own /chat endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_url: str = \"http://localhost:8000/chat\", model_version: str = \"gpt-4.1-2025-04-14\"):\n",
    "        \"\"\"\n",
    "        Initialize the custom LLM API client.\n",
    "        Args:\n",
    "            api_url: The URL of your /chat endpoint.\n",
    "            model_version: The model version to use.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.model_version = model_version\n",
    "\n",
    "    def chat(self, messages: List[dict], max_tokens: int = 100, temperature: float = 0.2) -> str:\n",
    "        \"\"\"\n",
    "        Send a chat completion request to the custom API.\n",
    "        Args:\n",
    "            messages: List of message dicts (role/content) for the conversation.\n",
    "            max_tokens: Maximum tokens to generate.\n",
    "            temperature: Sampling temperature.\n",
    "        Returns:\n",
    "            The generated answer as a string.\n",
    "        \"\"\"\n",
    "        # Compose the prompt from the message history\n",
    "        system_prompt = \"\"\n",
    "        user_prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_prompt += msg[\"content\"] + \"\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                user_prompt += msg[\"content\"] + \"\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                user_prompt += \"Assistant: \" + msg[\"content\"] + \"\\n\"\n",
    "\n",
    "        payload = {\n",
    "            \"system_prompt\": system_prompt.strip(),\n",
    "            \"user_prompt\": user_prompt.strip(),\n",
    "            \"model_version\": self.model_version,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "        response = requests.post(self.api_url, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"content\"]\n",
    "\n",
    "# --- Dummy Retriever (replace with your own retriever for production) ---\n",
    "\n",
    "def retrieve_documents(query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Dummy retriever for demonstration.\n",
    "    Replace with your actual retriever (e.g., FAISS, ElasticSearch, etc.).\n",
    "    Args:\n",
    "        query: The query string.\n",
    "        top_k: Number of top documents to retrieve.\n",
    "    Returns:\n",
    "        List of relevant document strings.\n",
    "    \"\"\"\n",
    "    knowledge_base = {\n",
    "        \"Paris\": \"Paris is the capital city of France.\",\n",
    "        \"France\": \"France is a country in Europe.\",\n",
    "        \"Eiffel Tower\": \"The Eiffel Tower is a landmark in Paris.\",\n",
    "        \"Europe\": \"Europe is a continent that includes France.\",\n",
    "    }\n",
    "    results = []\n",
    "    for k, v in knowledge_base.items():\n",
    "        if k.lower() in query.lower():\n",
    "            results.append(v)\n",
    "    return results[:top_k]\n",
    "\n",
    "# --- Chain-of-Thought RAG using Custom API ---\n",
    "\n",
    "def chain_of_thought_rag(query: str):\n",
    "    \"\"\"\n",
    "    Chain-of-Thought RAG pipeline using a custom LLM API.\n",
    "    - Performs step-by-step reasoning, retrieving supporting documents at each step.\n",
    "    - At each step, the LLM generates a thought, and the retriever fetches relevant facts.\n",
    "    - The process repeats for a fixed number of steps or until no more facts are found.\n",
    "    - Finally, the LLM is asked to synthesize a final answer based on the reasoning chain and retrieved facts.\n",
    "    Args:\n",
    "        query: The user question.\n",
    "    \"\"\"\n",
    "    llm = CustomAPILLM()  # Initialize the custom LLM client\n",
    "\n",
    "    # Step 1: Initial step-by-step reasoning prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers questions step by step, using facts you are provided.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{query} Let's think step by step.\"},\n",
    "    ]\n",
    "\n",
    "    for step in range(3):\n",
    "        # Step 2: Model generates the next step/thought\n",
    "        assistant_message = llm.chat(messages, max_tokens=100, temperature=0.2)\n",
    "        print(f\"Step {step+1} LLM: {assistant_message}\")\n",
    "\n",
    "        # Step 3: Retrieve supporting documents based on the current thought\n",
    "        retrieved_docs = retrieve_documents(assistant_message)\n",
    "        if not retrieved_docs:\n",
    "            break  # No more info found, stop reasoning\n",
    "\n",
    "        # Step 4: Add retrieved knowledge as \"system\" messages for the next step\n",
    "        for doc in retrieved_docs:\n",
    "            messages.append({\"role\": \"system\", \"content\": f\"Relevant information: {doc}\"})\n",
    "\n",
    "        # Step 5: Continue reasoning by appending the assistant's thought\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "    # Step 6: Ask for the final answer based on the above reasoning and facts\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Based on the above, what is the final answer?\"})\n",
    "    final_answer = llm.chat(messages, max_tokens=100, temperature=0.2)\n",
    "    print(\"Final Answer:\", final_answer)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    chain_of_thought_rag(\"Where is the Eiffel Tower located?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2b181",
   "metadata": {},
   "source": [
    "Minimal Knowledge Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f702c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 LLM: Sure! Let's reason step by step using the knowledge graph:\n",
      "\n",
      "1. Eiffel Tower --[located in]--> Paris  \n",
      "2. Paris --[located in]--> France\n",
      "\n",
      "Therefore, the Eiffel Tower is in France.\n",
      "Retrieved from KG:\n",
      " - Eiffel Tower --[located_in]--> Paris\n",
      "Step 2 LLM: Since we know the Eiffel Tower is in France, we can infer more about its geographical or cultural context. For example:\n",
      "\n",
      "- France --[located in]--> Europe\n",
      "\n",
      "So, the Eiffel Tower is also located in Europe.\n",
      "\n",
      "If you want to know more, you could ask about its history, significance, or other landmarks in Paris or France!\n",
      "Retrieved from KG:\n",
      " - Paris --[is_capital_of]--> France\n",
      "Final Answer: Given all these facts, the final answer is:\n",
      "\n",
      "The Eiffel Tower is in France.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example: Toy Knowledge Graph (triples)\n",
    "KG = [\n",
    "    (\"Eiffel Tower\", \"located_in\", \"Paris\"),\n",
    "    (\"Paris\", \"is_capital_of\", \"France\"),\n",
    "    (\"France\", \"in_continent\", \"Europe\"),\n",
    "    (\"Eiffel Tower\", \"type\", \"Landmark\"),\n",
    "    (\"Europe\", \"type\", \"Continent\"),\n",
    "]\n",
    "\n",
    "def query_kg(subject=None, predicate=None, obj=None):\n",
    "    \"\"\"\n",
    "    Find all triples in the knowledge graph matching the given pattern.\n",
    "    Args:\n",
    "        subject: Subject entity to match (or None for wildcard).\n",
    "        predicate: Predicate/relation to match (or None for wildcard).\n",
    "        obj: Object entity to match (or None for wildcard).\n",
    "    Returns:\n",
    "        List of matching (subject, predicate, object) triples.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for (s, p, o) in KG:\n",
    "        if (subject is None or s == subject) and (predicate is None or p == predicate) and (obj is None or o == obj):\n",
    "            results.append((s, p, o))\n",
    "    return results\n",
    "\n",
    "def format_kg_triplets_for_prompt(triplets):\n",
    "    \"\"\"\n",
    "    Format a list of KG triples for inclusion in an LLM prompt.\n",
    "    Args:\n",
    "        triplets: List of (subject, predicate, object) tuples.\n",
    "    Returns:\n",
    "        String representation for the prompt.\n",
    "    \"\"\"\n",
    "    if not triplets:\n",
    "        return \"No relevant facts found.\"\n",
    "    return \"\\n\".join([f\"- {s} --[{p}]--> {o}\" for s, p, o in triplets])\n",
    "\n",
    "class CustomAPILLM:\n",
    "    \"\"\"\n",
    "    Custom LLM class that uses a remote API to generate answers.\n",
    "    This replaces local LLMs (like OpenAI) with a call to your own /chat endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_url: str = \"http://localhost:8000/chat\", model_version: str = \"gpt-4.1-2025-04-14\"):\n",
    "        \"\"\"\n",
    "        Initialize the custom LLM API client.\n",
    "        Args:\n",
    "            api_url: The URL of your /chat endpoint.\n",
    "            model_version: The model version to use.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.model_version = model_version\n",
    "\n",
    "    def chat(self, messages, max_tokens=100, temperature=0.2):\n",
    "        \"\"\"\n",
    "        Send a chat completion request to the custom API.\n",
    "        Args:\n",
    "            messages: List of message dicts (role/content) for the conversation.\n",
    "            max_tokens: Maximum tokens to generate.\n",
    "            temperature: Sampling temperature.\n",
    "        Returns:\n",
    "            The generated answer as a string.\n",
    "        \"\"\"\n",
    "        # Compose the prompt from the message history\n",
    "        system_prompt = \"\"\n",
    "        user_prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_prompt += msg[\"content\"] + \"\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                user_prompt += msg[\"content\"] + \"\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                user_prompt += \"Assistant: \" + msg[\"content\"] + \"\\n\"\n",
    "\n",
    "        payload = {\n",
    "            \"system_prompt\": system_prompt.strip(),\n",
    "            \"user_prompt\": user_prompt.strip(),\n",
    "            \"model_version\": self.model_version,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "        response = requests.post(self.api_url, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"content\"]\n",
    "\n",
    "def kg_rag_chain_of_thought(question):\n",
    "    \"\"\"\n",
    "    Minimal Knowledge Graph RAG pipeline using a custom LLM API.\n",
    "    - Performs step-by-step reasoning, retrieving supporting facts from a knowledge graph at each step.\n",
    "    - At each step, the LLM generates a thought, and the retriever fetches relevant KG triples.\n",
    "    - The process repeats for a fixed number of steps or until no more facts are found.\n",
    "    - Finally, the LLM is asked to synthesize a final answer based on the reasoning chain and retrieved facts.\n",
    "    Args:\n",
    "        question: The user question.\n",
    "    \"\"\"\n",
    "    llm = CustomAPILLM()  # Initialize the custom LLM client\n",
    "\n",
    "    # Step 1: Initial user question with structured reasoning prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers questions step by step using facts from a knowledge graph. Each fact is presented as Subject --[Predicate]--> Object.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\\nLet's reason with the knowledge graph step by step.\"},\n",
    "    ]\n",
    "\n",
    "    # Step 2: Ask LLM what to look for first (could also be automated)\n",
    "    assistant_message = llm.chat(messages, max_tokens=100, temperature=0.2)\n",
    "    print(\"Step 1 LLM:\", assistant_message)\n",
    "\n",
    "    # Step 3: Retrieve relevant KG facts based on LLM's thought (for demo, hardcoded for Eiffel Tower)\n",
    "    # In production, parse assistant_message to extract entity/relation of interest\n",
    "    retrieved_facts = query_kg(subject=\"Eiffel Tower\", predicate=\"located_in\")\n",
    "    kg_facts = format_kg_triplets_for_prompt(retrieved_facts)\n",
    "    print(\"Retrieved from KG:\\n\", kg_facts)\n",
    "\n",
    "    # Step 4: Add KG facts and continue reasoning\n",
    "    messages.extend([\n",
    "        {\"role\": \"assistant\", \"content\": assistant_message},\n",
    "        {\"role\": \"system\", \"content\": f\"Facts from the KG:\\n{kg_facts}\"},\n",
    "        {\"role\": \"user\", \"content\": \"What can we infer next?\"},\n",
    "    ])\n",
    "    second_message = llm.chat(messages, max_tokens=100, temperature=0.2)\n",
    "    print(\"Step 2 LLM:\", second_message)\n",
    "\n",
    "    # Step 5: Retrieve more KG facts (e.g., about Paris)\n",
    "    retrieved_facts2 = query_kg(subject=\"Paris\")\n",
    "    kg_facts2 = format_kg_triplets_for_prompt(retrieved_facts2)\n",
    "    print(\"Retrieved from KG:\\n\", kg_facts2)\n",
    "\n",
    "    # Step 6: Continue chain and ask for final answer\n",
    "    messages.extend([\n",
    "        {\"role\": \"assistant\", \"content\": second_message},\n",
    "        {\"role\": \"system\", \"content\": f\"More facts from the KG:\\n{kg_facts2}\"},\n",
    "        {\"role\": \"user\", \"content\": \"Given all these facts, what is the final answer?\"},\n",
    "    ])\n",
    "    final_response = llm.chat(messages, max_tokens=100, temperature=0.2)\n",
    "    print(\"Final Answer:\", final_response)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    kg_rag_chain_of_thought(\"Which country is the Eiffel Tower in?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
